import os
import json
from typing import List, Dict, Any, Annotated
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

from agents.base_agent import BaseAgent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# å®šä¹‰å›¾ï¼ˆGraphï¼‰çš„çŠ¶æ€ï¼Œå®ƒå°†è´¯ç©¿æ•´ä¸ªå·¥ä½œæµ
class AgentState(TypedDict):
    # messages åˆ—è¡¨ç”¨äºåœ¨ä¸åŒ Agent ä¹‹é—´ä¼ é€’ä¿¡æ¯
    messages: Annotated[list, add_messages]
    # åŸå§‹é—®é¢˜ï¼Œæ–¹ä¾¿éšæ—¶å›æº¯
    original_question: str
    # çº¯å¤§æ¨¡å‹å›ç­”ç»“æœ
    llm_response: str
    # æœç´¢å·¥å…·å›ç­”ç»“æœ
    search_response: str
    # RAGå·¥å…·å›ç­”ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    rag_response: str
    # å¯ç”¨çš„å·¥å…·åˆ—è¡¨
    available_tools: list
    # æ˜¯å¦æœ‰RAGå·¥å…·
    has_rag_tool: bool


class LangGraphCoTTreeAgent(BaseAgent):
    """
    ä½¿ç”¨æ ‘çŠ¶å¹¶è¡Œå·¥ä½œæµçš„ CoTï¼ˆChain-of-Thoughtï¼‰Agentã€‚
    å·¥ä½œæµ: ç”¨æˆ·è¯·æ±‚ -> [çº¯LLMå›ç­” | æœç´¢å·¥å…·å›ç­” | RAGå·¥å…·å›ç­”] (å¹¶è¡Œ) -> æ€»ç»“æ±‡æ€»
    """

    @property
    def framework(self) -> str:
        return "LangGraph"

    @property
    def name(self) -> str:
        return "langgraph_cot_tree_agent"

    @property
    def display_name(self) -> str:
        return "CoT ç”Ÿæˆ Agent (æ ‘çŠ¶å¹¶è¡Œæ‰§è¡Œ)"

    @property
    def description(self) -> str:
        return "é€šè¿‡å¤šè·¯å¹¶è¡Œæ‰§è¡Œï¼ˆçº¯LLMã€æœç´¢å·¥å…·ã€RAGå·¥å…·ï¼‰çš„æ ‘çŠ¶å·¥ä½œæµï¼Œç”ŸæˆåŒ…å«å®Œæ•´æ€è€ƒè¿‡ç¨‹çš„ CoT æ•°æ®ã€‚"

    def _load_mcp_config(self) -> Dict[str, Any]:
        """
        ä» config.json æ–‡ä»¶åŠ è½½ MCP é…ç½®
        """
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ frontend/config.json è·¯å¾„
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
            config_path = os.path.join(project_root, "frontend", "config.json")
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… æˆåŠŸä» {config_path} åŠ è½½ MCP é…ç½®")
                return config
            else:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½ MCP é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def _categorize_tools(self, tools: list) -> Dict[str, list]:
        """
        å°†å·¥å…·æŒ‰ç±»å‹åˆ†ç±»
        """
        search_tools = []
        rag_tools = []
        other_tools = []
        
        for tool in tools:
            tool_name = tool.name.lower() if hasattr(tool, 'name') else str(tool).lower()
            print(tool_name)
            # tool_desc = tool.description.lower() if hasattr(tool, 'description') else str(tool).lower()
            
        
            # è¯†åˆ«æœç´¢å·¥å…·
            if any(keyword in tool_name for keyword in ['get', 'search', 'web', 'google', 'bing', 'æœç´¢']):
                search_tools.append(tool)
            # è¯†åˆ«RAGå·¥å…·
            elif any(keyword in tool_name for keyword in ['rag', 'retrieval', 'knowledge', 'document', 'vector', 'æ£€ç´¢', 'çŸ¥è¯†åº“']):
                rag_tools.append(tool)
            else:
                other_tools.append(tool)
        
        return {
            'search': search_tools,
            'rag': rag_tools,
            'other': other_tools
        }

    async def run(self, message: List[Dict[str, Any]], model: str, conversation_id: str) -> str:
        """
        æ‰§è¡Œ LangGraph Tree Agent çš„ä»»åŠ¡ã€‚
        """
        print(f"--- ğŸŒ³ Running LangGraph Tree Agent for conversation: {conversation_id} ---")
        
        try:
            user_question = message[-1]["content"]

            mcp_config = self._load_mcp_config()
            if not mcp_config:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ MCP é…ç½®ï¼Œè¯·åœ¨å‰ç«¯ç•Œé¢é…ç½® MCP å·¥å…·åå†è¯•ã€‚"
            
            print(f"ğŸ“‹ å½“å‰ MCP é…ç½®: {list(mcp_config.keys())}")

            client = MultiServerMCPClient(mcp_config)
            tools = await client.get_tools()
            
            # åˆ†ç±»å·¥å…·
            categorized_tools = self._categorize_tools(tools)
            print(f"ğŸ”§ å·¥å…·åˆ†ç±»ç»“æœ: æœç´¢å·¥å…· {len(categorized_tools['search'])} ä¸ª, RAGå·¥å…· {len(categorized_tools['rag'])} ä¸ª, å…¶ä»–å·¥å…· {len(categorized_tools['other'])} ä¸ª")
            
            llm = ChatOpenAI(model_name=os.getenv("MODEL", model), temperature=0)

            # 1. çº¯å¤§æ¨¡å‹å›ç­” Agent ğŸ¤–
            pure_llm_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ ğŸ¤–ã€‚è¯·ä»…åŸºäºä½ çš„å†…ç½®çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                ä¸è¦ä½¿ç”¨ä»»ä½•å¤–éƒ¨å·¥å…·ï¼Œåªä¾é ä½ çš„è®­ç»ƒæ•°æ®å’Œæ¨ç†èƒ½åŠ›ã€‚
                è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®ä¸”æœ‰å¸®åŠ©çš„å›ç­”ã€‚
                """,
                tools=[]  # ä¸æä¾›ä»»ä½•å·¥å…·
            )

            # 2. æœç´¢å·¥å…·å›ç­” Agent ğŸ”
            search_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸“å®¶ ğŸ”ã€‚è¯·ä½¿ç”¨æœç´¢å·¥å…·æ¥æŸ¥æ‰¾æœ€æ–°ã€æœ€å‡†ç¡®çš„ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                ä¼˜å…ˆä½¿ç”¨æœç´¢å·¥å…·è·å–å®æ—¶ä¿¡æ¯ï¼Œç„¶ååŸºäºæœç´¢ç»“æœæä¾›å…¨é¢çš„å›ç­”ã€‚
                """,
                tools=categorized_tools['search'] + categorized_tools['other']  # æä¾›æœç´¢å·¥å…·å’Œå…¶ä»–å·¥å…·
            )
            
            # 3. RAGå·¥å…·å›ç­” Agent ğŸ“š (å¦‚æœæœ‰RAGå·¥å…·)
            rag_agent = None
            if categorized_tools['rag']:
                rag_agent = create_react_agent(
                    model=llm,
                    prompt="""
                    ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ£€ç´¢ä¸“å®¶ ğŸ“šã€‚è¯·ä½¿ç”¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å·¥å…·æ¥æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£å’ŒçŸ¥è¯†åº“ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                    ä¼˜å…ˆä½¿ç”¨æ£€ç´¢å·¥å…·è·å–ç›¸å…³æ–‡æ¡£ï¼Œç„¶ååŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯æä¾›å‡†ç¡®çš„å›ç­”ã€‚
                    """,
                    tools=categorized_tools['rag'] + categorized_tools['other']  # æä¾›RAGå·¥å…·å’Œå…¶ä»–å·¥å…·
                )

            # 4. æ€»ç»“æ±‡æ€» Agent âœï¸
            summary_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªæ€»ç»“æ±‡æ€»ä¸“å®¶ âœï¸ã€‚ä½ å°†æ”¶åˆ°æ¥è‡ªä¸åŒæ¥æºçš„å›ç­”ï¼š
                1. çº¯å¤§æ¨¡å‹å›ç­”ï¼ˆåŸºäºå†…ç½®çŸ¥è¯†ï¼‰
                2. æœç´¢å·¥å…·å›ç­”ï¼ˆåŸºäºå®æ—¶æœç´¢ï¼‰
                3. RAGå·¥å…·å›ç­”ï¼ˆåŸºäºçŸ¥è¯†åº“æ£€ç´¢ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰
                
                è¯·ç»¼åˆè¿™äº›ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€å‡†ç¡®ä¸”ç»“æ„åŒ–çš„æœ€ç»ˆå›ç­”ã€‚
                
                è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»„ç»‡ä½ çš„å›ç­”ï¼š
                
                ## ğŸŒŸ ç»¼åˆåˆ†æä¸æœ€ç»ˆç­”æ¡ˆ ğŸŒŸ
                
                ### ğŸ“Š ä¿¡æ¯æ¥æºåˆ†æ
                *ç®€è¦è¯´æ˜å„ä¸ªæ¥æºæä¾›çš„ä¿¡æ¯ç‰¹ç‚¹å’Œä»·å€¼*
                
                ### âœ… æœ€ç»ˆç­”æ¡ˆ
                *åŸºäºæ‰€æœ‰ä¿¡æ¯æºçš„ç»¼åˆåˆ†æï¼Œæä¾›æœ€å‡†ç¡®ã€æœ€å…¨é¢çš„ç­”æ¡ˆ*
                
                ### ğŸ” è¡¥å……è¯´æ˜
                *å¦‚æœ‰å¿…è¦ï¼Œæä¾›é¢å¤–çš„èƒŒæ™¯ä¿¡æ¯æˆ–æ³¨æ„äº‹é¡¹*
                
                è¯·ç¡®ä¿å›ç­”å†…å®¹è¯¦å®ã€é€»è¾‘æ¸…æ™°ã€è¯­è¨€æµç•…ä¸“ä¸šã€‚
                """,
                tools=[]
            )

            workflow = StateGraph(AgentState)

            async def run_pure_llm(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ¤– çº¯å¤§æ¨¡å‹å›ç­” ---")
                try:
                    response = await pure_llm_agent.ainvoke({"messages": [HumanMessage(content=state['original_question'])]})
                    llm_response = response['messages'][-1].content
                    print(f"ğŸ¤– çº¯å¤§æ¨¡å‹å›ç­”å®Œæˆ: {len(llm_response)} å­—ç¬¦")
                    return {"llm_response": llm_response, "messages": response['messages']}
                except Exception as e:
                    print(f"âŒ çº¯å¤§æ¨¡å‹å›ç­”å¤±è´¥: {e}")
                    return {"llm_response": f"çº¯å¤§æ¨¡å‹å›ç­”å¤±è´¥: {str(e)}", "messages": state['messages']}

            async def run_search_tools(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ” æœç´¢å·¥å…·å›ç­” ---")
                try:
                    if not categorized_tools['search']:
                        search_response = "æ²¡æœ‰å¯ç”¨çš„æœç´¢å·¥å…·"
                    else:
                        response = await search_agent.ainvoke({"messages": [HumanMessage(content=state['original_question'])]})
                        search_response = response['messages'][-1].content
                    print(f"ğŸ” æœç´¢å·¥å…·å›ç­”å®Œæˆ: {len(search_response)} å­—ç¬¦")
                    return {"search_response": search_response, "messages": state['messages']}
                except Exception as e:
                    print(f"âŒ æœç´¢å·¥å…·å›ç­”å¤±è´¥: {e}")
                    return {"search_response": f"æœç´¢å·¥å…·å›ç­”å¤±è´¥: {str(e)}", "messages": state['messages']}

            async def run_rag_tools(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ“š RAGå·¥å…·å›ç­” ---")
                try:
                    if not categorized_tools['rag']:
                        rag_response = "æ²¡æœ‰å¯ç”¨çš„RAGå·¥å…·"
                    else:
                        response = await rag_agent.ainvoke({"messages": [HumanMessage(content=state['original_question'])]})
                        rag_response = response['messages'][-1].content
                    print(f"ğŸ“š RAGå·¥å…·å›ç­”å®Œæˆ: {len(rag_response)} å­—ç¬¦")
                    return {"rag_response": rag_response, "messages": state['messages']}
                except Exception as e:
                    print(f"âŒ RAGå·¥å…·å›ç­”å¤±è´¥: {e}")
                    return {"rag_response": f"RAGå·¥å…·å›ç­”å¤±è´¥: {str(e)}", "messages": state['messages']}

            async def run_summary(state: AgentState):
                print("--- èŠ‚ç‚¹: âœï¸ æ€»ç»“æ±‡æ€» ---")
                
                # æ„å»ºæ€»ç»“æç¤º
                summary_prompt = f"""
                åŸå§‹é—®é¢˜: {state['original_question']}
                
                ä»¥ä¸‹æ˜¯æ¥è‡ªä¸åŒæ¥æºçš„å›ç­”ï¼Œè¯·è¿›è¡Œç»¼åˆåˆ†æï¼š
                
                **ğŸ¤– çº¯å¤§æ¨¡å‹å›ç­”:**
                {state.get('llm_response', 'æ— ')}
                
                **ğŸ” æœç´¢å·¥å…·å›ç­”:**
                {state.get('search_response', 'æ— ')}
                
                **ğŸ“š RAGå·¥å…·å›ç­”:**
                {state.get('rag_response', 'æ— ')}
                
                è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆå›ç­”ã€‚
                """
                
                try:
                    response = await summary_agent.ainvoke({"messages": [HumanMessage(content=summary_prompt)]})
                    print("âœ… æ€»ç»“æ±‡æ€»å®Œæˆ")
                    return {"messages": response['messages']}
                except Exception as e:
                    print(f"âŒ æ€»ç»“æ±‡æ€»å¤±è´¥: {e}")
                    # å¦‚æœæ€»ç»“å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„æ±‡æ€»
                    fallback_summary = f"""
                    ## ğŸŒŸ ç»¼åˆåˆ†æä¸æœ€ç»ˆç­”æ¡ˆ ğŸŒŸ
                    
                    ### âŒ æ€»ç»“è¿‡ç¨‹å‡ºç°é”™è¯¯
                    æ€»ç»“Agentæ‰§è¡Œå¤±è´¥: {str(e)}
                    
                    ### ğŸ“Š å„æ¥æºå›ç­”
                    
                    **ğŸ¤– çº¯å¤§æ¨¡å‹å›ç­”:**
                    {state.get('llm_response', 'æ— ')}
                    
                    **ğŸ” æœç´¢å·¥å…·å›ç­”:**
                    {state.get('search_response', 'æ— ')}
                    
                    **ğŸ“š RAGå·¥å…·å›ç­”:**
                    {state.get('rag_response', 'æ— ')}
                    """
                    return {"messages": [HumanMessage(content=fallback_summary)]}

            # æ·»åŠ èŠ‚ç‚¹
            workflow.add_node("pure_llm_agent", run_pure_llm)
            workflow.add_node("search_agent", run_search_tools)
            workflow.add_node("rag_agent", run_rag_tools)
            workflow.add_node("summary_agent", run_summary)
            
            # è®¾ç½®å¹¶è¡Œæ‰§è¡Œçš„å·¥ä½œæµç¨‹
            workflow.set_entry_point("pure_llm_agent")
            workflow.set_entry_point("search_agent")
            workflow.set_entry_point("rag_agent")
            
            # æ‰€æœ‰å¹¶è¡ŒèŠ‚ç‚¹éƒ½è¿æ¥åˆ°æ€»ç»“èŠ‚ç‚¹
            workflow.add_edge("pure_llm_agent", "summary_agent")
            workflow.add_edge("search_agent", "summary_agent")
            workflow.add_edge("rag_agent", "summary_agent")
            workflow.add_edge("summary_agent", END)

            app = workflow.compile()
            print("ğŸš€ Starting LangGraph Tree Workflow...")
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "messages": [HumanMessage(content=user_question)],
                "original_question": user_question,
                "llm_response": "",
                "search_response": "",
                "rag_response": "",
                "available_tools": tools,
                "has_rag_tool": len(categorized_tools['rag']) > 0
            }
            
            final_state = await app.ainvoke(initial_state, debug=False)
            
            print("âœ… Tree Workflow finished.")
            final_response = final_state['messages'][-1].content
            print("Final Response:", final_response)

            return final_response

        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ LangGraph Tree Agent æ‰§è¡Œå¤±è´¥ï¼\n\n" \
                   f"**é”™è¯¯ä¿¡æ¯**: {str(e)}\n\n" \
                   f"è¯·æ£€æŸ¥æ‚¨çš„ MCP æœåŠ¡é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"