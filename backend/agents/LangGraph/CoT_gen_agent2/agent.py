import os
import json
from typing import List, Dict, Any, Annotated
from typing_extensions import TypedDict
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

from agents.base_agent import BaseAgent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# å®šä¹‰ä¸€ä¸ªå¸¸é‡æ¥é™åˆ¶æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
MAX_LOOPS = 7

# å®šä¹‰å›¾ï¼ˆGraphï¼‰çš„çŠ¶æ€ï¼Œå®ƒå°†è´¯ç©¿æ•´ä¸ªå·¥ä½œæµ
class AgentState(TypedDict):
    # messages åˆ—è¡¨ç”¨äºåœ¨ä¸åŒ Agent ä¹‹é—´ä¼ é€’ä¿¡æ¯
    messages: Annotated[list, add_messages]
    # åŸå§‹é—®é¢˜ï¼Œæ–¹ä¾¿éšæ—¶å›æº¯
    original_question: str
    # é—®é¢˜åˆ†æçš„ç»“æœ
    analysis: str
    # åˆ¶å®šçš„è®¡åˆ’
    plan: str
    # å°†æ‰§è¡Œå†å²è®°å½•ä¸ºç®€å•çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä»¥æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    execution_history: list[str]
    # [NEW] å¢åŠ ä¸€ä¸ªè®¡æ•°å™¨æ¥è·Ÿè¸ªå¾ªç¯æ¬¡æ•°
    loop_count: int


class LangGraphCoTAgent(BaseAgent):
    """
    ä½¿ç”¨è‡ªå®šä¹‰çš„å›¾ï¼ˆGraphï¼‰å·¥ä½œæµç”Ÿæˆä¸€ä¸ªåŒ…å«å®Œæ•´æ€è€ƒè¿‡ç¨‹çš„ CoTï¼ˆChain-of-Thoughtï¼‰é•¿æ€ç»´é“¾ã€‚
    å·¥ä½œæµ: åˆ†æ -> è®¡åˆ’ -> [æ‰§è¡Œ <-> åˆ¤æ–­] (å¾ªç¯) -> æ€»ç»“
    """

    @property
    def framework(self) -> str:
        return "LangGraph"

    @property
    def name(self) -> str:
        return "langgraph_cot_agent_custom_flow"

    @property
    def display_name(self) -> str:
        return "CoT ç”Ÿæˆ Agent (å¸¦çº¦æŸçš„è‡ªå®šä¹‰å¾ªç¯)"

    @property
    def description(self) -> str:
        return "é€šè¿‡åˆ†æã€è®¡åˆ’ã€å¾ªç¯æ‰§è¡Œå’Œæ€»ç»“çš„è‡ªå®šä¹‰å·¥ä½œæµï¼Œç”ŸæˆåŒ…å«å®Œæ•´æ€è€ƒè¿‡ç¨‹çš„ CoT æ•°æ®ã€‚"

    def _load_mcp_config(self) -> Dict[str, Any]:
        """
        ä» config.json æ–‡ä»¶åŠ è½½ MCP é…ç½®
        """
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ frontend/config.json è·¯å¾„
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
            config_path = os.path.join(project_root, "frontend", "config.json")
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… æˆåŠŸä» {config_path} åŠ è½½ MCP é…ç½®")
                return config
            else:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½ MCP é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    async def run(self, message: List[Dict[str, Any]], model: str, conversation_id: str) -> str:
        """
        æ‰§è¡Œ LangGraph Agent çš„ä»»åŠ¡ã€‚
        """
        print(f"--- ğŸƒâ€â™‚ï¸ Running Custom LangGraph Agent for conversation: {conversation_id} ---")
        
        try:
            user_question = message[-1]["content"]

            mcp_config = self._load_mcp_config()
            if not mcp_config:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ MCP é…ç½®ï¼Œè¯·åœ¨å‰ç«¯ç•Œé¢é…ç½® MCP å·¥å…·åå†è¯•ã€‚"
            
            print(f"ğŸ“‹ å½“å‰ MCP é…ç½®: {list(mcp_config.keys())}")

            client = MultiServerMCPClient(mcp_config)
            tools = await client.get_tools()
            
            llm = ChatOpenAI(model_name=os.getenv("MODEL", model), temperature=0)

            # 1. é—®é¢˜åˆ†æ Agent ğŸ§
            analysis_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªé—®é¢˜åˆ†æä¸“å®¶ ğŸ§ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ·±å…¥å‰–æç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œè¯†åˆ«å…¶æ ¸å¿ƒï¼Œå¹¶æ¸…æ™°åœ°ç½—åˆ—å‡ºä½ çš„åˆ†æç»“æœã€‚è¯·ä¿æŒå›ç­”ç®€æ´æ‰¼è¦ã€‚
                """,
                tools=[]
            )

            # 2. è®¡åˆ’åˆ¶å®š Agent ğŸ“
            planning_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªè®¡åˆ’åˆ¶å®šä¸“å®¶ ğŸ“ã€‚æ ¹æ®é—®é¢˜åˆ†æç»“æœï¼Œåˆ¶å®šä¸€ä¸ªæ¸…æ™°ã€åˆ†æ­¥éª¤çš„è¡ŒåŠ¨è®¡åˆ’ã€‚
                é‡è¦æç¤ºï¼šæ•´ä¸ªè®¡åˆ’ã€ä¸è¦è¶…è¿‡5ä¸ªæ­¥éª¤ã€‘ï¼Œå¹¶ç¡®ä¿æ¯ä¸€æ­¥éƒ½å…·ä½“å¯æ“ä½œã€‚
                """,
                tools=[]
            )
            
            # 3. è®¡åˆ’æ‰§è¡Œ Agent ğŸ› ï¸
            execution_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªä¸€ä¸ä¸è‹Ÿçš„è®¡åˆ’æ‰§è¡Œè€… ğŸ› ï¸ã€‚ä¸¥æ ¼æŒ‰ç…§è®¡åˆ’å’Œç°æœ‰å†å²ï¼Œä½¿ç”¨å·¥å…·æ‰§è¡Œã€ä¸‹ä¸€ä¸ªæœªå®Œæˆã€‘çš„æ­¥éª¤ã€‚
                è¯·ä¸€æ¬¡åªæ‰§è¡Œã€ä¸€ä¸ªã€‘æ­¥éª¤ï¼Œå¹¶ç®€è¦æŠ¥å‘Šä½ çš„æ“ä½œå’Œç»“æœã€‚
                """,
                tools=tools
            )

            # 4. åˆ¤æ–­ Agent ğŸ¤”
            judgement_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªåˆ¤æ–­ä¸“å®¶ ğŸ¤”ã€‚æŸ¥çœ‹è®¡åˆ’å’Œæ‰§è¡Œå†å²ï¼Œåˆ¤æ–­è®¡åˆ’æ˜¯å¦å·²å…¨éƒ¨å®Œæˆã€‚
                - å¦‚æœå·²å®Œæˆï¼Œåªå›ç­” `finished`ã€‚
                - å¦‚æœæœªå®Œæˆï¼Œåªå›ç­” `continue`ã€‚
                ä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„è§£é‡Šã€‚
                """,
                tools=[]
            )

            # 5. æ€»ç»“ Agent âœï¸
            summarization_agent = create_react_agent(
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªæ€»ç»“æŠ¥å‘Šä¸“å®¶ âœï¸ã€‚åŸºäºæ•´ä¸ªå·¥ä½œæµç¨‹ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢è€Œè¯¦ç»†çš„æœ€ç»ˆæŠ¥å‘Šã€‚
                è¯·åŠ¡å¿…æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç»„ç»‡ä½ çš„æŠ¥å‘Šï¼š

                ## ğŸŒŸ æˆ‘çš„æ€è€ƒä¸æ‰§è¡Œå…¨è¿‡ç¨‹ ğŸŒŸ

                ### 1. ğŸ¤” é—®é¢˜åˆ†æ
                *æœ€åˆæˆ‘æ˜¯è¿™æ ·ç†è§£è¿™ä¸ªé—®é¢˜çš„...*

                ### 2. ï¿½ è¡ŒåŠ¨è®¡åˆ’
                *ä¸ºæ­¤ï¼Œæˆ‘åˆ¶å®šäº†å¦‚ä¸‹çš„è¡ŒåŠ¨è®¡åˆ’...*

                ### 3. ğŸ› ï¸ æ‰§è¡Œè¿‡ç¨‹
                *æˆ‘æ˜¯è¿™æ ·ä¸€æ­¥æ­¥æ‰§è¡Œçš„...ï¼ˆè¯·åœ¨è¿™é‡Œæ•´åˆæ‰€æœ‰æ‰§è¡Œæ­¥éª¤å’Œç»“æœï¼‰*

                ### 4. âœ… æœ€ç»ˆç­”æ¡ˆ
                *åŸºäºä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œæˆ‘çš„æœ€ç»ˆç»“è®ºæ˜¯...*

                è¯·ç¡®ä¿æŠ¥å‘Šå†…å®¹è¯¦å®ã€æ ¼å¼ä¼˜ç¾ã€è¯­è¨€æµç•…ä¸“ä¸šã€‚ç›´æ¥è¾“å‡ºè¿™ä»½æŠ¥å‘Šã€‚
                """,
                tools=[]
            )

            workflow = StateGraph(AgentState)

            async def run_analysis(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ§ é—®é¢˜åˆ†æ ---")
                response = await analysis_agent.ainvoke(state)
                return {"analysis": response['messages'][-1].content, "messages": response['messages']}

            async def run_planning(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ“ è®¡åˆ’åˆ¶å®š ---")
                prompt = f"è¿™æ˜¯é—®é¢˜åˆ†æçš„ç»“æœï¼Œè¯·åŸºäºæ­¤åˆ¶å®šè®¡åˆ’ï¼š\n\n{state['analysis']}"
                response = await planning_agent.ainvoke({"messages": [HumanMessage(content=prompt)]})
                return {"plan": response['messages'][-1].content, "messages": response['messages']}

            async def run_execution(state: AgentState):
                print(f"--- èŠ‚ç‚¹: ğŸ› ï¸ è®¡åˆ’æ‰§è¡Œ (å¾ªç¯ #{state.get('loop_count', 0) + 1}) ---")
                history_str = "\n".join(state.get('execution_history', []))
                prompt = f"""
                è¿™æ˜¯è®¡åˆ’å’Œå†å²ï¼Œè¯·æ‰§è¡Œä¸‹ä¸€æ­¥:
                è®¡åˆ’: {state['plan']}
                å†å²: {history_str}
                """
                response = await execution_agent.ainvoke({"messages": [HumanMessage(content=prompt)]})
                
                new_history_entry = response['messages'][-1].content
                new_history = state.get('execution_history', []) + [new_history_entry]
                
                return {
                    "execution_history": new_history, 
                    "messages": response['messages'],
                    "loop_count": state.get('loop_count', 0) + 1 # [MODIFIED] æ›´æ–°å¾ªç¯è®¡æ•°
                }

            async def run_judgement(state: AgentState):
                print("--- èŠ‚ç‚¹: ğŸ¤” åˆ¤æ–­ ---")
                history_str = "\n".join(state.get('execution_history', []))
                prompt = f"""
                åˆ¤æ–­è®¡åˆ’æ˜¯å¦å®Œæˆã€‚
                è®¡åˆ’: {state['plan']}
                å†å²: {history_str}
                """
                response = await judgement_agent.ainvoke({"messages": [HumanMessage(content=prompt)]})
                return {"messages": response['messages']}

            async def run_summary(state: AgentState):
                print("--- èŠ‚ç‚¹: âœï¸ æ€»ç»“æŠ¥å‘Š ---")
                history_str = "\n".join(f"- {step}" for step in state.get('execution_history', []))
                
                termination_reason = ""
                if state.get('loop_count', 0) >= MAX_LOOPS:
                    termination_reason = "**æ³¨æ„ï¼šç”±äºè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼Œä»»åŠ¡è¢«æå‰ç»ˆæ­¢ã€‚ä»¥ä¸‹æ˜¯åˆ°ç›®å‰ä¸ºæ­¢çš„ç»“æœæ€»ç»“ã€‚**\n\n"

                prompt = f"""
                {termination_reason}æ•´ä¸ªä»»åŠ¡æµç¨‹å·²ç»ç»“æŸï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆçš„è¯¦ç»†æŠ¥å‘Šã€‚

                **åŸå§‹é—®é¢˜**: {state['original_question']}
                **é—®é¢˜åˆ†æ**: {state['analysis']}
                **è¡ŒåŠ¨è®¡åˆ’**: {state['plan']}
                **å®Œæ•´æ‰§è¡Œå†å²**:
{history_str}
                """
                response = await summarization_agent.ainvoke({"messages": [HumanMessage(content=prompt)]})
                return {"messages": response['messages']}

            workflow.add_node("analysis_agent", run_analysis)
            workflow.add_node("planning_agent", run_planning)
            workflow.add_node("execution_agent", run_execution)
            workflow.add_node("judgement_agent", run_judgement)
            workflow.add_node("summarization_agent", run_summary)
            
            workflow.set_entry_point("analysis_agent")
            workflow.add_edge("analysis_agent", "planning_agent")
            workflow.add_edge("planning_agent", "execution_agent")
            
            # [MODIFIED] æ›´æ–°åˆ¤æ–­é€»è¾‘ä»¥åŒ…å«æœ€å¤§å¾ªç¯æ£€æŸ¥
            def should_continue(state: AgentState):
                last_message = state['messages'][-1].content.strip().lower()
                loop_count = state.get('loop_count', 0)
                print(f"--- åˆ¤æ–­ç»“æœ: '{last_message}', å½“å‰å¾ªç¯æ¬¡æ•°: {loop_count} ---")

                if loop_count >= MAX_LOOPS:
                    print(f"--- ğŸš« è¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° ({MAX_LOOPS})ï¼Œå¼ºåˆ¶ç»“æŸã€‚ ---")
                    return "summarization_agent"
                
                if "finished" in last_message:
                    return "summarization_agent"
                else:
                    return "execution_agent"

            workflow.add_conditional_edges(
                "judgement_agent",
                should_continue,
                {"execution_agent": "execution_agent", "summarization_agent": "summarization_agent"}
            )
            
            workflow.add_edge("execution_agent", "judgement_agent")
            workflow.add_edge("summarization_agent", END)

            app = workflow.compile()
            print("ğŸš€ Starting LangGraph Custom Workflow...")
            
            # [MODIFIED] åˆå§‹åŒ–çŠ¶æ€ï¼ŒåŒ…æ‹¬ loop_count
            initial_state = {
                "messages": [HumanMessage(content=user_question)],
                "original_question": user_question,
                "execution_history": [],
                "loop_count": 0,
            }
            final_state = await app.ainvoke(initial_state)
            
            print("âœ… Workflow finished.")
            final_response = final_state['messages'][-1].content
            print("Final Response:", final_response)

            return final_response

        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ LangGraph Agent æ‰§è¡Œå¤±è´¥ï¼\n\n" \
                   f"**é”™è¯¯ä¿¡æ¯**: {str(e)}\n\n" \
                   f"è¯·æ£€æŸ¥æ‚¨çš„ MCP æœåŠ¡é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"
