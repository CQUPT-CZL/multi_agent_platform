import os
import json
from typing import List, Dict, Any
from agents.base_agent import BaseAgent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph_supervisor.handoff import create_forward_message_tool
from langgraph_supervisor import create_supervisor
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

class LangGraphCoTAgent(BaseAgent):
    """
    è®²ä¸€ä¸ªé—®é¢˜ç”ŸæˆCoTé•¿æ€ç»´é“¾
    """

    @property
    def framework(self) -> str:
        return "LangGraph"

    @property
    def name(self) -> str:
        return "langgraph_cot_agent"

    @property
    def display_name(self) -> str:
        return "ä½¿ç”¨supervisoræ¨¡å¼ç”ŸæˆCoT"

    @property
    def description(self) -> str:
        return "ä½¿ç”¨supervisoræ¨¡å¼ç”ŸæˆCoTé•¿æ€ç»´é“¾æ•°æ®"

    def _load_mcp_config(self) -> Dict[str, Any]:
        """
        ä» config.json æ–‡ä»¶åŠ è½½ MCP é…ç½®
        """
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ frontend/config.json è·¯å¾„
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
            config_path = os.path.join(project_root, "frontend", "config.json")
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… æˆåŠŸä» {config_path} åŠ è½½ MCP é…ç½®")
                return config
            else:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½ MCP é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    async def run(self, message: List[Dict[str, Any]], model: str, conversation_id: str) -> str:
        """
        æ‰§è¡Œ LangGraph Agent çš„ä»»åŠ¡ã€‚
        """
        print(f"--- Running LangGraph MCP Agent for conversation: {conversation_id} ---")
        
        try:
            user_question = message[-1]["content"]

            # ä» config.json åŠ è½½ MCP é…ç½®
            mcp_config = self._load_mcp_config()
            
            if not mcp_config:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ MCP é…ç½®ï¼Œè¯·åœ¨å‰ç«¯ç•Œé¢é…ç½® MCP å·¥å…·åå†è¯•ã€‚"
            
            print(f"ğŸ“‹ å½“å‰ MCP é…ç½®: {list(mcp_config.keys())}")

            client = MultiServerMCPClient(mcp_config)

            tools = await client.get_tools()
            
            # åˆ›å»ºä¸€ä¸ªChatOpenAIå®ä¾‹
            llm = ChatOpenAI(model_name=os.getenv("MODEL"), temperature=0)

            # 1. é—®é¢˜åˆ†æ Agent
            analysis_agent = create_react_agent(
                name="analysis_agent",
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªé—®é¢˜åˆ†æä¸“å®¶ï¼Œè´Ÿè´£å°†é—®é¢˜è¿›è¡Œåˆ†è§£ï¼Œçœ‹çœ‹æœ‰ä»€ä¹ˆæ¦‚å¿µæ˜¯ä½ ä¸çŸ¥é“çš„ï¼Œæˆ–è€…éœ€è¦æ¾„æ¸…çš„ï¼Œéƒ½æŒ‡å‡ºæ¥ã€‚
                """,
                tools=[]
            )

            # 2. è®¡åˆ’åˆ¶å®š Agent
            planning_agent = create_react_agent(
                name="planning_agent",
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªä¸“é—¨åšè®¡åˆ’çš„ä¸“å®¶ã€‚æ ¹æ®é—®é¢˜çš„åˆ†æç»“æœï¼Œåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„ã€åˆ†æ­¥éª¤çš„è¡ŒåŠ¨è®¡åˆ’ï¼Œè¯´æ˜ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜éœ€è¦åšä»€ä¹ˆï¼Œæ¯”å¦‚æŸ¥é˜…å“ªäº›èµ„æ–™ç­‰ã€‚
                """,
                tools=[]
            )
            
            # 3. è®¡åˆ’æ‰§è¡Œ Agent (æ–°å¢)
            execution_agent = create_react_agent(
                name="execution_agent",
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªè®¡åˆ’æ‰§è¡Œä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è®¡åˆ’ï¼Œä¸€æ­¥ä¸€æ­¥åœ°ä½¿ç”¨å·¥å…·æ¥æ‰§è¡Œã€‚
                ä½ éœ€è¦è¯¦ç»†è®°å½•æ¯ä¸€æ­¥çš„æ‰§è¡Œè¿‡ç¨‹å’Œäº§å‡ºçš„ç»“æœã€‚ä½ æœ‰ä¸€äº›å·¥å…·ä¾›ä½ ä½¿ç”¨ã€‚
                """,
                tools=tools
            )

            # 4. ç­”æ¡ˆæ•´åˆ Agent (æ–°å¢)
            integration_agent = create_react_agent(
                name="integration_agent",
                model=llm,
                prompt="""
                ä½ æ˜¯ä¸€ä¸ªæ•´åˆä¸“å®¶ã€‚æ ¹æ®è®¡åˆ’æ‰§è¡Œçš„æ‰€æœ‰ç»“æœï¼Œè¿›è¡Œå…¨é¢åœ°åˆ†æå’Œæ€»ç»“ã€‚
                ä½ çš„ç›®æ ‡æ˜¯å½¢æˆä¸€ä¸ªå®Œæ•´ã€æµç•…ã€ä¸“ä¸šä¸”æ ¼å¼ä¼˜ç¾çš„æœ€ç»ˆç­”æ¡ˆã€‚
                è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™çš„æ€è€ƒè¿‡ç¨‹æˆ–è§£é‡Šã€‚
                """,
                tools=[]
            )


            # åˆ›å»ºåŒ…å«å››ä¸ª agent çš„ supervisor å·¥ä½œæµ
            forwarding_tool = create_forward_message_tool()
            workflow = create_supervisor(
                [analysis_agent, planning_agent, execution_agent, integration_agent],
                model=llm,
                # å°†æ–°å·¥å…·æ·»åŠ åˆ° supervisor å¯ç”¨å·¥å…·åˆ—è¡¨ä¸­
                tools=[forwarding_tool],
                prompt=(
                    "ä½ æ˜¯ä¸€ä¸ªå›¢é˜Ÿæ€»ç›‘ ğŸ‘¨â€ğŸ’¼ï¼Œè´Ÿè´£åè°ƒä¸€ä¸ªç”±å››åä¸“å®¶ç»„æˆçš„ç²¾è‹±å›¢é˜Ÿæ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n\n"
                    "ä½ çš„å›¢é˜Ÿæˆå‘˜åŒ…æ‹¬ï¼š\n"
                    "1.  **é—®é¢˜åˆ†æä¸“å®¶ (analysis_agent)** ğŸ§: è´Ÿè´£æ·±å…¥åˆ†æå’Œåˆ†è§£ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚\n"
                    "2.  **è®¡åˆ’åˆ¶å®šä¸“å®¶ (planning_agent)** ğŸ“: æ ¹æ®é—®é¢˜åˆ†æç»“æœï¼Œåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„ã€åˆ†æ­¥éª¤çš„è¡ŒåŠ¨è®¡åˆ’ã€‚\n"
                    "3.  **è®¡åˆ’æ‰§è¡Œä¸“å®¶ (execution_agent)** ğŸ› ï¸: ä¸¥æ ¼æŒ‰ç…§è®¡åˆ’ï¼Œä½¿ç”¨å¯ç”¨å·¥å…·æ‰§è¡Œæ¯ä¸€ä¸ªæ­¥éª¤ï¼Œå¹¶æ”¶é›†ç»“æœã€‚\n"
                    "4.  **ç­”æ¡ˆæ•´åˆä¸“å®¶ (integration_agent)** âœï¸: å°†æ‰€æœ‰æ‰§è¡Œç»“æœæ•´åˆæˆä¸€ä¸ªå…¨é¢ã€æ¸…æ™°ä¸”è¿è´¯çš„æœ€ç»ˆç­”æ¡ˆã€‚\n\n"
                    "**å·¥ä½œæµç¨‹** ğŸš€:\n"
                    "1.  ä½ é¦–å…ˆå°†ç”¨æˆ·çš„é—®é¢˜å‘ç»™ã€é—®é¢˜åˆ†æä¸“å®¶ã€‘è¿›è¡Œæ‹†è§£ã€‚\n"
                    "2.  ç„¶åï¼Œå°†åˆ†æç»“æœä¼ é€’ç»™ã€è®¡åˆ’åˆ¶å®šä¸“å®¶ã€‘æ¥åˆ›å»ºè¡ŒåŠ¨è®¡åˆ’ã€‚\n"
                    "3.  æ¥ä¸‹æ¥ï¼Œå°†åˆ¶å®šçš„è®¡åˆ’äº¤ç»™ã€è®¡åˆ’æ‰§è¡Œä¸“å®¶ã€‘æ¥ä»˜è¯¸è¡ŒåŠ¨ã€‚\n"
                    "4.  ç„¶åï¼Œå°†æ‰€æœ‰çš„æ‰§è¡Œæˆæœäº¤ç»™ã€ç­”æ¡ˆæ•´åˆä¸“å®¶ã€‘è¿›è¡Œæœ€åçš„æ•´ç†å’Œæ¶¦è‰²ã€‚\n"
                    "5.  **æœ€åï¼Œå½“ã€ç­”æ¡ˆæ•´åˆä¸“å®¶ã€‘å®Œæˆåï¼Œä½ å¿…é¡»è°ƒç”¨ `forward_message` å·¥å…·ï¼Œå¹¶å°† `from_agent` è®¾ç½®ä¸º `integration_agent`**ï¼Œä»¥ç›´æ¥å°†æœ€ç»ˆç­”æ¡ˆä½œä¸ºè¾“å‡ºã€‚è¿™å¯ä»¥èŠ‚çœæˆæœ¬å¹¶ä¿è¯ç»“æœçš„åŸæ±åŸå‘³ã€‚\n\n"
                    "è¯·ä¸¥æ ¼æŒ‰ç…§æ­¤æµç¨‹è¿›è¡Œï¼Œå°¤å…¶æ˜¯åœ¨æœ€åä¸€æ­¥ä½¿ç”¨ `forward_message` å·¥å…·ã€‚"
                )
            )

            # ç¼–è¯‘å¹¶è¿è¡Œå·¥ä½œæµ
            app = workflow.compile()
            print("ğŸš€ Starting LangGraph Supervisor Workflow...")
            response = await app.ainvoke({
                "messages": [
                    {
                        "role": "user",
                        "content": user_question
                    }
                ]
            })
            print("âœ… Workflow finished.")
            print("Final Response:", response)

            # è¿”å›æœ€ç»ˆç”± supervisor æ•´åˆè¿‡çš„å†…å®¹
            return response["messages"][-1].content

        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ LangGraph MCP Agent æ‰§è¡Œå¤±è´¥ï¼\n\n" \
                   f"**é”™è¯¯ä¿¡æ¯**: {str(e)}\n\n" \
                   f"è¯·æ£€æŸ¥æ‚¨çš„ MCP æœåŠ¡é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"
