import os
import json
from typing import List, Dict, Any
# [MODIFIED] ä¸å†éœ€è¦å¤æ‚çš„ State å’Œå›¾ç»“æ„ï¼Œå¯¼å…¥ç®€åŒ–
from langchain_core.messages import HumanMessage

from agents.base_agent import BaseAgent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

# [REFACTORED] Agent ç±»è¢«å¤§å¹…ç®€åŒ–
class LangGraphReactAgent(BaseAgent):
    """
    ä¸€ä¸ªå¼ºå¤§çš„ã€è‡ªä¸»å†³ç­–çš„ ReAct Agentã€‚
    å®ƒèƒ½æ ¹æ®å†…éƒ¨æ€è€ƒï¼Œè‡ªç”±å†³å®šæ˜¯å¦ã€ä»¥åŠå¦‚ä½•ä½¿ç”¨ `search` å’Œ `rag_search` å·¥å…·ï¼Œä¹Ÿå¯èƒ½å¤šæ¬¡ä½¿ç”¨æˆ–ä¸ä½¿ç”¨ã€‚
    """

    @property
    def framework(self) -> str:
        return "LangGraph"

    @property
    def name(self) -> str:
        return "autonomous_react_agent"

    @property
    def display_name(self) -> str:
        return "è‡ªä¸»å†³ç­– Agent (ReAct æ¨¡å¼)"

    @property
    def description(self) -> str:
        return "ä¸€ä¸ªèƒ½è‡ªä¸»æ€è€ƒå¹¶å†³å®šå¦‚ä½•ä½¿ç”¨ Search/RAG å·¥å…·çš„å¼ºå¤§ä»£ç†ã€‚"

    def _load_mcp_config(self) -> Dict[str, Any]:
        """
        ä» config.json æ–‡ä»¶åŠ è½½ MCP é…ç½®
        """
        try:
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
            config_path = os.path.join(project_root, "frontend", "config.json")
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… æˆåŠŸä» {config_path} åŠ è½½ MCP é…ç½®")
                return config
            else:
                print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½ MCP é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    # [REFACTORED] run æ–¹æ³•è¢«å¤§å¹…ç®€åŒ–
    async def run(self, message: List[Dict[str, Any]], model: str, conversation_id: str) -> str:
        """
        æ‰§è¡Œè‡ªä¸»å†³ç­–çš„ ReAct Agentã€‚
        """
        print(f"--- ğŸƒâ€â™‚ï¸ Running Autonomous ReAct Agent for conversation: {conversation_id} ---")
        
        try:
            user_question = message[-1]["content"]

            # 1. åŠ è½½å·¥å…·ç®±
            mcp_config = self._load_mcp_config()
            if not mcp_config:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ MCP é…ç½®ï¼Œè¯·åœ¨å‰ç«¯ç•Œé¢é…ç½® MCP å·¥å…·åå†è¯•ã€‚"
            
            print(f"ğŸ“‹ å½“å‰ MCP é…ç½®: {list(mcp_config.keys())}")
            client = MultiServerMCPClient(mcp_config)
            tools = await client.get_tools()
            print(f"ğŸ› ï¸ Agent å¯ç”¨å·¥å…·: {[tool.name for tool in tools]}")
            
            # 2. åˆå§‹åŒ– LLM
            llm = ChatOpenAI(model_name=os.getenv("MODEL", model), temperature=0)

            # [CRITICAL] 3. å®šä¹‰èµ‹äºˆ Agent è‡ªä¸»å†³ç­–èƒ½åŠ›çš„â€œå¤§è„‘â€ -> ç³»ç»Ÿæç¤º (System Prompt)
            # è¿™æ˜¯æ•´ä¸ª Agent çš„çµé­‚æ‰€åœ¨
            MASTER_PROMPT = """
            ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„é—®é¢˜è§£å†³ä¸“å®¶å’Œç ”ç©¶å‘˜ï¼Œèƒ½åŠ›å¼ºå¤§ä¸”è¡Œäº‹ä¸¥è°¨ã€‚

            **ä½ çš„å·¥ä½œæµç¨‹æ˜¯**:
            1.  **æ€è€ƒ**: åœ¨æ¯ä¸€æ­¥ï¼Œä½ éƒ½å¿…é¡»å…ˆè¿›è¡Œæ€è€ƒï¼ˆThoughtï¼‰ã€‚åˆ†æå½“å‰æƒ…å†µï¼Œåˆ¤æ–­ä½ æ˜¯å¦å·²ç»æŒæ¡äº†è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”æœ€ç»ˆé—®é¢˜ã€‚
            2.  **è¡ŒåŠ¨**: å¦‚æœä½ éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå°±é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„å·¥å…·ï¼ˆActionï¼‰æ¥è·å–ã€‚å¦‚æœä½ è®¤ä¸ºä¿¡æ¯å·²ç»è¶³å¤Ÿï¼Œä½ çš„è¡ŒåŠ¨å°±æ˜¯ç›´æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

            **ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·æœ‰**:
            - `search`: ç”¨äºè¿›è¡Œé€šç”¨çš„ã€å¼€æ”¾å¼çš„ç½‘ç»œæœç´¢ã€‚å½“ä½ éœ€è¦äº†è§£ä¸€ä¸ªæ¦‚å¿µçš„èƒŒæ™¯ã€å®šä¹‰æˆ–æŸ¥æ‰¾å®½æ³›ä¿¡æ¯æ—¶ä½¿ç”¨å®ƒã€‚
            - `rag_search`: ç”¨äºè¿›è¡Œä¸“ä¸šçš„ã€æ·±å…¥çš„çŸ¥è¯†åº“æ£€ç´¢ã€‚å½“ä½ éœ€è¦ç²¾å‡†çš„æŠ€æœ¯ç»†èŠ‚ã€è¡Œä¸šæ•°æ®æˆ–ç‰¹å®šé¢†åŸŸçš„æ·±åº¦åŸç†æ—¶ä½¿ç”¨å®ƒã€‚

            **ä½ çš„å†³ç­–é€»è¾‘**:
            - **æˆ‘æ˜¯å¦éœ€è¦å·¥å…·?** å¦‚æœé—®é¢˜å¾ˆç®€å•ï¼Œæˆ–è€…åŸºäºå†å²ä¿¡æ¯ä½ å·²ç»çŸ¥é“äº†ç­”æ¡ˆï¼Œå°±ä¸è¦ä½¿ç”¨ä»»ä½•å·¥å…·ï¼Œç›´æ¥æ€è€ƒå¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
            - **è¯¥ç”¨å“ªä¸ªå·¥å…·?** æ ¹æ®é—®é¢˜çš„æ€§è´¨ï¼Œåœ¨ `search` å’Œ `rag_search` ä¸­åšå‡ºæœ€ä½³é€‰æ‹©ã€‚
            - **æˆ‘éœ€è¦å¤šæ¬¡ä½¿ç”¨å·¥å…·å—?** å®Œå…¨å¯ä»¥ï¼ä½ å¯èƒ½ä¼šå…ˆç”¨ `search` è·å¾—èƒŒæ™¯çŸ¥è¯†ï¼Œç„¶åæ ¹æ®èƒŒæ™¯çŸ¥è¯†ï¼Œå†ç”¨ `rag_search` æ·±å…¥æŒ–æ˜ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç­–ç•¥ã€‚
            - **ä»€ä¹ˆæ—¶å€™åœæ­¢?** å½“ä½ é€šè¿‡æ€è€ƒï¼ˆThoughtï¼‰åˆ¤æ–­ï¼Œä½ ä»å·¥å…·ä¸­è·å¾—çš„è§‚å¯Ÿï¼ˆObservationï¼‰å·²ç»è¶³å¤Ÿå…¨é¢ï¼Œèƒ½å¤Ÿå®Œç¾å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ—¶ï¼Œä½ å°±åº”è¯¥åœæ­¢ä½¿ç”¨å·¥å…·ï¼Œå¹¶æä¾›ä½ çš„æœ€ç»ˆç­”æ¡ˆï¼ˆFinal Answerï¼‰ã€‚

            **é‡è¦**: ä½ çš„è¾“å‡ºå¿…é¡»ä¸¥æ ¼éµå¾ª ReAct æ ¼å¼ï¼Œå³åŒ…å« "Thought:" å’Œ "Action:"ã€‚
            ç°åœ¨ï¼Œå¼€å§‹è§£å†³é—®é¢˜å§ï¼
            """

            # 4. ä½¿ç”¨ LangGraph çš„é¢„æ„å»ºåŠŸèƒ½åˆ›å»º ReAct Agent æ‰§è¡Œå™¨
            # è¿™ä¼šå¤„ç†æ‰€æœ‰çš„â€œæ€è€ƒ->è¡ŒåŠ¨->è§‚å¯Ÿâ€å¾ªç¯ï¼Œæˆ‘ä»¬ä¸å†éœ€è¦æ‰‹åŠ¨æ„å»ºå›¾
            agent_executor = create_react_agent(model=llm, tools=tools, prompt=MASTER_PROMPT, debug=True)

            print("ğŸš€ Starting Autonomous ReAct Workflow...")
            
            # 5. æ‰§è¡Œ Agent å¹¶è·å–æœ€ç»ˆç»“æœ
            # ä½¿ç”¨éæµå¼è¾“å‡ºï¼Œç›´æ¥è·å–å®Œæ•´çš„å“åº”ç»“æœ
            result = await agent_executor.ainvoke(
                {"messages": [HumanMessage(content=user_question)]}
            )
            
            # è·å–æœ€ç»ˆå“åº”
            final_response = result["messages"][-1].content
            
            print("âœ… Workflow finished.")
            print("Final Response:", final_response)

            return final_response

        except Exception as e:
            import traceback
            traceback.print_exc()
            return f"âŒ LangGraph Agent æ‰§è¡Œå¤±è´¥ï¼\n\n" \
                   f"**é”™è¯¯ä¿¡æ¯**: {str(e)}\n\n" \
                   f"è¯·æ£€æŸ¥æ‚¨çš„ MCP æœåŠ¡é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚"